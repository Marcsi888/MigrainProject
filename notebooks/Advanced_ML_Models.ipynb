{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (400, 24)\n",
      "No missing values found in the dataset\n",
      "Number of duplicate rows: 6\n",
      "Duplicate rows:\n",
      "     Age  Duration  Frequency  Location  Character  Intensity  Nausea  Vomit  \\\n",
      "94    28         1          5         1          1          2       1      0   \n",
      "118   28         1          5         1          1          2       1      0   \n",
      "169   31         1          1         1          1          2       1      1   \n",
      "200   50         1          1         1          1          3       1      0   \n",
      "280   22         1          1         1          1          2       1      0   \n",
      "281   35         1          1         1          1          3       1      0   \n",
      "\n",
      "     Phonophobia  Photophobia  ...  Vertigo  Tinnitus  Hypoacusis  Diplopia  \\\n",
      "94             1            1  ...        0         0           0         0   \n",
      "118            1            1  ...        0         0           0         0   \n",
      "169            1            1  ...        0         0           0         0   \n",
      "200            1            1  ...        0         0           0         0   \n",
      "280            1            1  ...        0         0           0         0   \n",
      "281            1            1  ...        0         0           0         0   \n",
      "\n",
      "     Defect  Ataxia  Conscience  Paresthesia  DPF                        Type  \n",
      "94        0       0           0            0    1  Typical aura with migraine  \n",
      "118       0       0           0            0    1  Typical aura with migraine  \n",
      "169       0       0           0            0    1  Typical aura with migraine  \n",
      "200       0       0           0            0    0  Typical aura with migraine  \n",
      "280       0       0           0            0    0  Typical aura with migraine  \n",
      "281       0       0           0            0    0  Typical aura with migraine  \n",
      "\n",
      "[6 rows x 24 columns]\n",
      "Shape after removing duplicates: (394, 24)\n",
      "Outliers in Age:\n",
      "Number of outliers: 4\n",
      "Bounds: [-5.0, 67.0]\n",
      "     Age                        Type\n",
      "35    68       Migraine without aura\n",
      "60    70       Migraine without aura\n",
      "67    69  Typical aura with migraine\n",
      "121   77  Typical aura with migraine\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')  # or './src' if running from project root\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from data_utils import load_migraine_data, check_missing_values, remove_duplicates, detect_outliers\n",
    "\n",
    "# Example usage:\n",
    "df = load_migraine_data('C://Users//nyolc//Downloads//migraine_data.csv')\n",
    "check_missing_values(df)\n",
    "df_clean = remove_duplicates(df)\n",
    "outliers, lb, ub = detect_outliers(df_clean, 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                           roc_auc_score, confusion_matrix, classification_report)\n",
    "\n",
    "# Model imports\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization setup\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 2. Data Loading and Initial Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Loading and exploring migraine dataset...\n",
      "\n",
      " Dataset Overview:\n",
      "   â€¢ Shape: (400, 24)\n",
      "   â€¢ Columns: ['Age', 'Duration', 'Frequency', 'Location', 'Character', 'Intensity', 'Nausea', 'Vomit', 'Phonophobia', 'Photophobia', 'Visual', 'Sensory', 'Dysphasia', 'Dysarthria', 'Vertigo', 'Tinnitus', 'Hypoacusis', 'Diplopia', 'Defect', 'Ataxia', 'Conscience', 'Paresthesia', 'DPF', 'Type']\n",
      "\n",
      " Data Types:\n",
      "Age             int64\n",
      "Duration        int64\n",
      "Frequency       int64\n",
      "Location        int64\n",
      "Character       int64\n",
      "Intensity       int64\n",
      "Nausea          int64\n",
      "Vomit           int64\n",
      "Phonophobia     int64\n",
      "Photophobia     int64\n",
      "Visual          int64\n",
      "Sensory         int64\n",
      "Dysphasia       int64\n",
      "Dysarthria      int64\n",
      "Vertigo         int64\n",
      "Tinnitus        int64\n",
      "Hypoacusis      int64\n",
      "Diplopia        int64\n",
      "Defect          int64\n",
      "Ataxia          int64\n",
      "Conscience      int64\n",
      "Paresthesia     int64\n",
      "DPF             int64\n",
      "Type           object\n",
      "dtype: object\n",
      "\n",
      " Missing Values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      " Summary Statistics:\n",
      "              Age    Duration   Frequency    Location   Character  Intensity  \\\n",
      "count  400.000000  400.000000  400.000000  400.000000  400.000000  400.00000   \n",
      "mean    31.705000    1.610000    2.365000    0.972500    0.977500    2.47000   \n",
      "std     12.139043    0.770964    1.675947    0.268186    0.277825    0.76849   \n",
      "min     15.000000    1.000000    1.000000    0.000000    0.000000    0.00000   \n",
      "25%     22.000000    1.000000    1.000000    1.000000    1.000000    2.00000   \n",
      "50%     28.000000    1.000000    2.000000    1.000000    1.000000    3.00000   \n",
      "75%     40.000000    2.000000    4.000000    1.000000    1.000000    3.00000   \n",
      "max     77.000000    3.000000    8.000000    2.000000    2.000000    3.00000   \n",
      "\n",
      "           Nausea       Vomit  Phonophobia  Photophobia  ...  Dysarthria  \\\n",
      "count  400.000000  400.000000   400.000000   400.000000  ...    400.0000   \n",
      "mean     0.987500    0.322500     0.977500     0.980000  ...      0.0025   \n",
      "std      0.111242    0.468019     0.148489     0.140175  ...      0.0500   \n",
      "min      0.000000    0.000000     0.000000     0.000000  ...      0.0000   \n",
      "25%      1.000000    0.000000     1.000000     1.000000  ...      0.0000   \n",
      "50%      1.000000    0.000000     1.000000     1.000000  ...      0.0000   \n",
      "75%      1.000000    1.000000     1.000000     1.000000  ...      0.0000   \n",
      "max      1.000000    1.000000     1.000000     1.000000  ...      1.0000   \n",
      "\n",
      "          Vertigo    Tinnitus  Hypoacusis    Diplopia      Defect  Ataxia  \\\n",
      "count  400.000000  400.000000  400.000000  400.000000  400.000000   400.0   \n",
      "mean     0.125000    0.060000    0.015000    0.005000    0.015000     0.0   \n",
      "std      0.331133    0.237784    0.121705    0.070622    0.121705     0.0   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000     0.0   \n",
      "25%      0.000000    0.000000    0.000000    0.000000    0.000000     0.0   \n",
      "50%      0.000000    0.000000    0.000000    0.000000    0.000000     0.0   \n",
      "75%      0.000000    0.000000    0.000000    0.000000    0.000000     0.0   \n",
      "max      1.000000    1.000000    1.000000    1.000000    1.000000     0.0   \n",
      "\n",
      "       Conscience  Paresthesia         DPF  \n",
      "count  400.000000   400.000000  400.000000  \n",
      "mean     0.017500     0.007500    0.410000  \n",
      "std      0.131289     0.086385    0.492449  \n",
      "min      0.000000     0.000000    0.000000  \n",
      "25%      0.000000     0.000000    0.000000  \n",
      "50%      0.000000     0.000000    0.000000  \n",
      "75%      0.000000     0.000000    1.000000  \n",
      "max      1.000000     1.000000    1.000000  \n",
      "\n",
      "[8 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def load_and_explore_data(filepath):\n",
    "    \"\"\"Load and perform comprehensive data exploration\"\"\"\n",
    "    print(\"ðŸ“Š Loading and exploring migraine dataset...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    print(f\"\\n Dataset Overview:\")\n",
    "    print(f\"   â€¢ Shape: {df.shape}\")\n",
    "    print(f\"   â€¢ Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\n Data Types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(f\"\\n Missing Values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    print(missing[missing > 0])\n",
    "    \n",
    "    print(f\"\\n Summary Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "df = load_and_explore_data('C://Users//nyolc//Downloads//migraine_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 3. Data Preprocessing and Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (312, 23), Test shape: (78, 23)\n"
     ]
    }
   ],
   "source": [
    "# --- 3.1 Handle Outliers (remove outliers in 'Age') ---\n",
    "# Remove outliers detected earlier (using bounds from detect_outliers)\n",
    "df_clean = df_clean[(df_clean['Age'] >= lb) & (df_clean['Age'] <= ub)]\n",
    "\n",
    "# --- 3.2 Encode Categorical Variables ---\n",
    "# Encode 'Type' (target) as numbers\n",
    "le = LabelEncoder()\n",
    "df_clean['Type_encoded'] = le.fit_transform(df_clean['Type'])\n",
    "\n",
    "# --- 3.3 Feature/Target Split ---\n",
    "X = df_clean.drop(['Type', 'Type_encoded'], axis=1)\n",
    "y = df_clean['Type_encoded']\n",
    "\n",
    "# --- 3.4 Feature Scaling ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# --- 3.5 Train-Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "CV F1 (weighted): 0.904 Â± 0.027\n",
      "Test Accuracy: 0.923\n",
      "Test F1 (weighted): 0.919\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "            Basilar-type aura       0.67      0.67      0.67         3\n",
      " Familial hemiplegic migraine       0.75      0.60      0.67         5\n",
      "        Migraine without aura       0.92      1.00      0.96        12\n",
      "                        Other       1.00      0.67      0.80         3\n",
      " Sporadic hemiplegic migraine       1.00      0.67      0.80         3\n",
      "   Typical aura with migraine       0.94      0.98      0.96        48\n",
      "Typical aura without migraine       1.00      1.00      1.00         4\n",
      "\n",
      "                     accuracy                           0.92        78\n",
      "                    macro avg       0.90      0.80      0.84        78\n",
      "                 weighted avg       0.92      0.92      0.92        78\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "CV F1 (weighted): 0.879 Â± 0.031\n",
      "Test Accuracy: 0.936\n",
      "Test F1 (weighted): 0.934\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "            Basilar-type aura       0.67      0.67      0.67         3\n",
      " Familial hemiplegic migraine       0.80      0.80      0.80         5\n",
      "        Migraine without aura       0.92      1.00      0.96        12\n",
      "                        Other       1.00      0.67      0.80         3\n",
      " Sporadic hemiplegic migraine       1.00      0.67      0.80         3\n",
      "   Typical aura with migraine       0.96      0.98      0.97        48\n",
      "Typical aura without migraine       1.00      1.00      1.00         4\n",
      "\n",
      "                     accuracy                           0.94        78\n",
      "                    macro avg       0.91      0.83      0.86        78\n",
      "                 weighted avg       0.94      0.94      0.93        78\n",
      "\n",
      "\n",
      "Training SVM...\n",
      "CV F1 (weighted): 0.875 Â± 0.021\n",
      "Test Accuracy: 0.910\n",
      "Test F1 (weighted): 0.908\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "            Basilar-type aura       0.67      0.67      0.67         3\n",
      " Familial hemiplegic migraine       0.60      0.60      0.60         5\n",
      "        Migraine without aura       0.92      1.00      0.96        12\n",
      "                        Other       1.00      0.67      0.80         3\n",
      " Sporadic hemiplegic migraine       1.00      0.67      0.80         3\n",
      "   Typical aura with migraine       0.94      0.96      0.95        48\n",
      "Typical aura without migraine       1.00      1.00      1.00         4\n",
      "\n",
      "                     accuracy                           0.91        78\n",
      "                    macro avg       0.88      0.79      0.83        78\n",
      "                 weighted avg       0.91      0.91      0.91        78\n",
      "\n",
      "\n",
      "Training KNN...\n",
      "CV F1 (weighted): 0.841 Â± 0.024\n",
      "Test Accuracy: 0.859\n",
      "Test F1 (weighted): 0.854\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "            Basilar-type aura       0.67      0.67      0.67         3\n",
      " Familial hemiplegic migraine       0.43      0.60      0.50         5\n",
      "        Migraine without aura       0.85      0.92      0.88        12\n",
      "                        Other       1.00      0.33      0.50         3\n",
      " Sporadic hemiplegic migraine       1.00      0.33      0.50         3\n",
      "   Typical aura with migraine       0.92      0.94      0.93        48\n",
      "Typical aura without migraine       1.00      1.00      1.00         4\n",
      "\n",
      "                     accuracy                           0.86        78\n",
      "                    macro avg       0.84      0.68      0.71        78\n",
      "                 weighted avg       0.88      0.86      0.85        78\n",
      "\n",
      "\n",
      "Training Naive Bayes...\n",
      "CV F1 (weighted): 0.931 Â± 0.030\n",
      "Test Accuracy: 0.923\n",
      "Test F1 (weighted): 0.919\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "            Basilar-type aura       0.67      0.67      0.67         3\n",
      " Familial hemiplegic migraine       0.75      0.60      0.67         5\n",
      "        Migraine without aura       0.92      1.00      0.96        12\n",
      "                        Other       1.00      0.67      0.80         3\n",
      " Sporadic hemiplegic migraine       1.00      0.67      0.80         3\n",
      "   Typical aura with migraine       0.94      0.98      0.96        48\n",
      "Typical aura without migraine       1.00      1.00      1.00         4\n",
      "\n",
      "                     accuracy                           0.92        78\n",
      "                    macro avg       0.90      0.80      0.84        78\n",
      "                 weighted avg       0.92      0.92      0.92        78\n",
      "\n",
      "\n",
      "Training Decision Tree...\n",
      "CV F1 (weighted): 0.816 Â± 0.019\n",
      "Test Accuracy: 0.846\n",
      "Test F1 (weighted): 0.839\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "            Basilar-type aura       0.00      0.00      0.00         3\n",
      " Familial hemiplegic migraine       0.44      0.80      0.57         5\n",
      "        Migraine without aura       0.92      1.00      0.96        12\n",
      "                        Other       0.50      0.33      0.40         3\n",
      " Sporadic hemiplegic migraine       1.00      0.67      0.80         3\n",
      "   Typical aura with migraine       0.91      0.90      0.91        48\n",
      "Typical aura without migraine       1.00      1.00      1.00         4\n",
      "\n",
      "                     accuracy                           0.85        78\n",
      "                    macro avg       0.68      0.67      0.66        78\n",
      "                 weighted avg       0.84      0.85      0.84        78\n",
      "\n",
      "\n",
      "Training Gradient Boosting...\n",
      "CV F1 (weighted): 0.888 Â± 0.038\n",
      "Test Accuracy: 0.897\n",
      "Test F1 (weighted): 0.898\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "            Basilar-type aura       0.67      0.67      0.67         3\n",
      " Familial hemiplegic migraine       0.50      0.60      0.55         5\n",
      "        Migraine without aura       0.92      1.00      0.96        12\n",
      "                        Other       1.00      0.67      0.80         3\n",
      " Sporadic hemiplegic migraine       1.00      0.67      0.80         3\n",
      "   Typical aura with migraine       0.94      0.94      0.94        48\n",
      "Typical aura without migraine       1.00      1.00      1.00         4\n",
      "\n",
      "                     accuracy                           0.90        78\n",
      "                    macro avg       0.86      0.79      0.82        78\n",
      "                 weighted avg       0.90      0.90      0.90        78\n",
      "\n",
      "\n",
      "Training AdaBoost...\n",
      "CV F1 (weighted): 0.648 Â± 0.079\n",
      "Test Accuracy: 0.718\n",
      "Test F1 (weighted): 0.636\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "            Basilar-type aura       1.00      0.33      0.50         3\n",
      " Familial hemiplegic migraine       0.00      0.00      0.00         5\n",
      "        Migraine without aura       0.67      0.17      0.27        12\n",
      "                        Other       1.00      0.33      0.50         3\n",
      " Sporadic hemiplegic migraine       0.00      0.00      0.00         3\n",
      "   Typical aura with migraine       0.70      1.00      0.82        48\n",
      "Typical aura without migraine       1.00      1.00      1.00         4\n",
      "\n",
      "                     accuracy                           0.72        78\n",
      "                    macro avg       0.62      0.40      0.44        78\n",
      "                 weighted avg       0.66      0.72      0.64        78\n",
      "\n",
      "\n",
      "Training Ridge Classifier...\n",
      "CV F1 (weighted): 0.855 Â± 0.015\n",
      "Test Accuracy: 0.897\n",
      "Test F1 (weighted): 0.883\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "            Basilar-type aura       1.00      0.67      0.80         3\n",
      " Familial hemiplegic migraine       0.75      0.60      0.67         5\n",
      "        Migraine without aura       0.92      0.92      0.92        12\n",
      "                        Other       1.00      0.33      0.50         3\n",
      " Sporadic hemiplegic migraine       1.00      0.33      0.50         3\n",
      "   Typical aura with migraine       0.89      1.00      0.94        48\n",
      "Typical aura without migraine       1.00      1.00      1.00         4\n",
      "\n",
      "                     accuracy                           0.90        78\n",
      "                    macro avg       0.94      0.69      0.76        78\n",
      "                 weighted avg       0.90      0.90      0.88        78\n",
      "\n",
      "\n",
      "Training MLP (Neural Net)...\n",
      "CV F1 (weighted): 0.897 Â± 0.029\n",
      "Test Accuracy: 0.910\n",
      "Test F1 (weighted): 0.912\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "            Basilar-type aura       0.67      0.67      0.67         3\n",
      " Familial hemiplegic migraine       0.57      0.80      0.67         5\n",
      "        Migraine without aura       0.92      1.00      0.96        12\n",
      "                        Other       1.00      0.67      0.80         3\n",
      " Sporadic hemiplegic migraine       1.00      0.67      0.80         3\n",
      "   Typical aura with migraine       0.96      0.94      0.95        48\n",
      "Typical aura without migraine       1.00      1.00      1.00         4\n",
      "\n",
      "                     accuracy                           0.91        78\n",
      "                    macro avg       0.87      0.82      0.83        78\n",
      "                 weighted avg       0.92      0.91      0.91        78\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_f1</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.878629</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.933788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.904368</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.919156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.930572</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.919156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP (Neural Net)</th>\n",
       "      <td>0.897132</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.911885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.875297</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.908279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.887671</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.898042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge Classifier</th>\n",
       "      <td>0.855150</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.883459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.841491</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.853796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.816107</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.838843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.648214</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.635700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        cv_f1  test_acc   test_f1\n",
       "Random Forest        0.878629  0.935897  0.933788\n",
       "Logistic Regression  0.904368  0.923077  0.919156\n",
       "Naive Bayes          0.930572  0.923077  0.919156\n",
       "MLP (Neural Net)     0.897132  0.910256  0.911885\n",
       "SVM                  0.875297  0.910256  0.908279\n",
       "Gradient Boosting    0.887671  0.897436  0.898042\n",
       "Ridge Classifier     0.855150  0.897436  0.883459\n",
       "KNN                  0.841491  0.858974  0.853796\n",
       "Decision Tree        0.816107  0.846154  0.838843\n",
       "AdaBoost             0.648214  0.717949  0.635700"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 4.1 Define Models ---\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"Ridge Classifier\": RidgeClassifier(),\n",
    "    \"MLP (Neural Net)\": MLPClassifier(max_iter=1000)\n",
    "}\n",
    "\n",
    "# --- 4.2 Cross-Validation and Training ---\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1_weighted')\n",
    "    print(f\"CV F1 (weighted): {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}\")\n",
    "    # Fit and evaluate on test set\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"Test Accuracy: {acc:.3f}\")\n",
    "    print(f\"Test F1 (weighted): {f1:.3f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    results[name] = {'cv_f1': cv_scores.mean(), 'test_acc': acc, 'test_f1': f1}\n",
    "\n",
    "# --- 4.3 Compare Results ---\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results).T\n",
    "display(results_df.sort_values('test_f1', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Results Visualization and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_results(results, y_test, le):\n",
    "    \"\"\"Create comprehensive visualizations of model results\"\"\"\n",
    "    print(\"Creating model comparison visualizations...\")\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    metrics_df = pd.DataFrame({\n",
    "        name: {\n",
    "            'Accuracy': data['test_acc'],\n",
    "            'F1 Score': data['test_f1'],\n",
    "            'CV F1': data['cv_f1']\n",
    "        }\n",
    "        for name, data in results.items()\n",
    "    }).T\n",
    "    \n",
    "    # Create visualization figure\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Model comparison bar plot\n",
    "    plt.subplot(3, 3, 1)\n",
    "    metrics_df[['Accuracy', 'F1 Score']].plot(kind='bar')\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.ylabel('Score')\n",
    "    \n",
    "    # 2. Cross-validation scores\n",
    "    plt.subplot(3, 3, 2)\n",
    "    cv_scores = [data['cv_f1'] for data in results.values()]\n",
    "    model_names = list(results.keys())\n",
    "    \n",
    "    plt.bar(range(len(model_names)), cv_scores)\n",
    "    plt.xticks(range(len(model_names)), model_names, rotation=45)\n",
    "    plt.title('Cross-Validation F1 Scores')\n",
    "    plt.ylabel('F1 Score')\n",
    "    \n",
    "    # 3. Accuracy vs F1 Score scatter\n",
    "    plt.subplot(3, 3, 3)\n",
    "    plt.scatter([data['test_acc'] for data in results.values()], \n",
    "                [data['test_f1'] for data in results.values()])\n",
    "    for i, name in enumerate(results.keys()):\n",
    "        plt.annotate(name, (list(results.values())[i]['test_acc'], \n",
    "                           list(results.values())[i]['test_f1']))\n",
    "    plt.xlabel('Test Accuracy')\n",
    "    plt.ylabel('Test F1 Score')\n",
    "    plt.title('Accuracy vs F1 Score')\n",
    "    \n",
    "    # 4-6. Confusion matrices for top 3 models\n",
    "    top_models = sorted(results.items(), key=lambda x: x[1]['test_f1'], reverse=True)[:3]\n",
    "    \n",
    "    for i, (name, data) in enumerate(top_models, 4):\n",
    "        plt.subplot(3, 3, i)\n",
    "        # Get the model and make predictions\n",
    "        model = models[name]\n",
    "        y_pred = model.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "        plt.title(f'Confusion Matrix - {name}')\n",
    "    \n",
    "    # 7. Feature importance (for Random Forest)\n",
    "    plt.subplot(3, 3, 7)\n",
    "    rf_model = models['Random Forest']\n",
    "    if hasattr(rf_model, 'feature_importances_'):\n",
    "        feature_names = [col for col in df_clean.columns if col not in ['Type', 'Type_encoded']]\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': rf_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=True).tail(10)\n",
    "        \n",
    "        plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "        plt.title('Top 10 Feature Importances (Random Forest)')\n",
    "    \n",
    "    # 8. Model ranking\n",
    "    plt.subplot(3, 3, 8)\n",
    "    ranking_df = metrics_df.rank(ascending=False).mean(axis=1).sort_values()\n",
    "    plt.barh(ranking_df.index, ranking_df.values)\n",
    "    plt.title('Overall Model Ranking (Lower is Better)')\n",
    "    \n",
    "    # 9. Performance distribution\n",
    "    plt.subplot(3, 3, 9)\n",
    "    metrics_df.boxplot()\n",
    "    plt.title('Performance Metrics Distribution')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Create visualizations\n",
    "metrics_summary = visualize_model_results(results, y_test, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance_summary(results):\n",
    "    \"\"\"Print comprehensive performance summary\"\"\"\n",
    "    print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sort models by F1 score\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['test_f1'], reverse=True)\n",
    "    \n",
    "    for rank, (name, data) in enumerate(sorted_results, 1):\n",
    "        print(f\"\\n{rank}. {name}\")\n",
    "        print(f\"   Accuracy:     {data['test_acc']:.4f}\")\n",
    "        print(f\"   F1 Score:     {data['test_f1']:.4f}\")\n",
    "        print(f\"   CV F1 Score:  {data['cv_f1']:.4f}\")\n",
    "    \n",
    "    # Best model analysis\n",
    "    best_model_name = sorted_results[0][0]\n",
    "    best_model_data = sorted_results[0][1]\n",
    "    \n",
    "    print(f\"\\nBEST PERFORMING MODEL: {best_model_name}\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"This model achieved the highest F1 score of {best_model_data['test_f1']:.4f}\")\n",
    "    print(f\"with a cross-validation F1 score of {best_model_data['cv_f1']:.4f}\")\n",
    "    \n",
    "    return best_model_name, best_model_data\n",
    "\n",
    "best_model_name, best_model_info = print_performance_summary(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_advanced_analysis(best_model_name, results, X_test, y_test, le):\n",
    "    \"\"\"Perform advanced analysis on the best model\"\"\"\n",
    "    print(f\"Advanced Analysis for {best_model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    best_model = models[best_model_name]\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # Feature importance analysis (if available)\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        print(f\"\\nTop 10 Most Important Features:\")\n",
    "        feature_names = [col for col in df_clean.columns if col not in ['Type', 'Type_encoded']]\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n",
    "            print(f\"   {i+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Error analysis\n",
    "    errors = X_test[y_test != y_pred]\n",
    "    if len(errors) > 0:\n",
    "        print(f\"\\nError Analysis:\")\n",
    "        print(f\"   Total misclassifications: {len(errors)}\")\n",
    "        print(f\"   Error rate: {len(errors)/len(y_test)*100:.2f}%\")\n",
    "    \n",
    "    # Class-wise performance\n",
    "    print(f\"\\nClass-wise Performance:\")\n",
    "    for i, class_name in enumerate(le.classes_):\n",
    "        class_mask = y_test == i\n",
    "        if class_mask.sum() > 0:\n",
    "            class_acc = (y_pred[class_mask] == y_test[class_mask]).mean()\n",
    "            print(f\"   {class_name}: {class_acc:.3f}\")\n",
    "\n",
    "perform_advanced_analysis(best_model_name, results, X_test, y_test, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Optimization for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_best_model(best_model_name, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Perform hyperparameter optimization for the best model\"\"\"\n",
    "    print(f\"Hyperparameter Optimization for {best_model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if best_model_name == \"Random Forest\":\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "        base_model = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    elif best_model_name == \"Logistic Regression\":\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga']\n",
    "        }\n",
    "        base_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    \n",
    "    elif best_model_name == \"SVM\":\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['rbf', 'linear'],\n",
    "            'gamma': ['scale', 'auto', 0.1, 0.01]\n",
    "        }\n",
    "        base_model = SVC(probability=True, random_state=42)\n",
    "    \n",
    "    else:\n",
    "        print(\"Hyperparameter optimization not implemented for this model type\")\n",
    "        return None\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        base_model, \n",
    "        param_grid, \n",
    "        cv=5, \n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate optimized model\n",
    "    y_pred_opt = grid_search.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_pred_opt)\n",
    "    test_f1 = f1_score(y_test, y_pred_opt, average='weighted')\n",
    "    \n",
    "    print(f\"Optimized model test accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Optimized model test F1: {test_f1:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "optimized_model = optimize_best_model(best_model_name, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Selection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_feature_selection(X_train, y_train, X_test, y_test, feature_names):\n",
    "    \"\"\"Perform feature selection analysis\"\"\"\n",
    "    print(\"Feature Selection Analysis\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Recursive Feature Elimination with Random Forest\n",
    "    from sklearn.feature_selection import RFE\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rfe = RFE(estimator=rf, n_features_to_select=10)\n",
    "    \n",
    "    X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    \n",
    "    # Train model with selected features\n",
    "    rf_selected = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_selected.fit(X_train_rfe, y_train)\n",
    "    \n",
    "    y_pred_rfe = rf_selected.predict(X_test_rfe)\n",
    "    acc_rfe = accuracy_score(y_test, y_pred_rfe)\n",
    "    f1_rfe = f1_score(y_test, y_pred_rfe, average='weighted')\n",
    "    \n",
    "    print(f\"Performance with top 10 features:\")\n",
    "    print(f\"   Accuracy: {acc_rfe:.4f}\")\n",
    "    print(f\"   F1 Score: {f1_rfe:.4f}\")\n",
    "    \n",
    "    # Show selected features\n",
    "    selected_features = [feature_names[i] for i in range(len(feature_names)) if rfe.support_[i]]\n",
    "    print(f\"\\nTop 10 selected features:\")\n",
    "    for i, feature in enumerate(selected_features, 1):\n",
    "        print(f\"   {i}. {feature}\")\n",
    "    \n",
    "    return selected_features, acc_rfe, f1_rfe\n",
    "\n",
    "selected_features, acc_rfe, f1_rfe = perform_feature_selection(X_train, y_train, X_test, y_test, \n",
    "                                                             [col for col in df_clean.columns if col not in ['Type', 'Type_encoded']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Interpretability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_interpretability(best_model_name, models, X_test, y_test, le):\n",
    "    \"\"\"Analyze model interpretability\"\"\"\n",
    "    print(\"Model Interpretability Analysis\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    best_model = models[best_model_name]\n",
    "    \n",
    "    # Feature importance plot\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        feature_names = [col for col in df_clean.columns if col not in ['Type', 'Type_encoded']]\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "        plt.title(f'Feature Importance - {best_model_name}')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Partial dependence plots for top features\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        from sklearn.inspection import partial_dependence\n",
    "        \n",
    "        top_features = [col for col in df_clean.columns if col not in ['Type', 'Type_encoded']][:3]\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, feature in enumerate(top_features, 1):\n",
    "            plt.subplot(1, 3, i)\n",
    "            try:\n",
    "                partial_dependence.plot_partial_dependence(\n",
    "                    best_model, X_test, [feature], \n",
    "                    feature_names=[col for col in df_clean.columns if col not in ['Type', 'Type_encoded']]\n",
    "                )\n",
    "                plt.title(f'Partial Dependence - {feature}')\n",
    "            except:\n",
    "                plt.text(0.5, 0.5, f'PDP not available\\nfor {feature}', \n",
    "                        ha='center', va='center', transform=plt.gca().transAxes)\n",
    "                plt.title(f'Partial Dependence - {feature}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "analyze_model_interpretability(best_model_name, models, X_test, y_test, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Recommendations and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_recommendations(metrics_summary, best_model_name, results):\n",
    "    \"\"\"Provide actionable recommendations based on results\"\"\"\n",
    "    print(\"RECOMMENDATIONS AND NEXT STEPS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(f\"Best Model: {best_model_name}\")\n",
    "    print(f\"   This model showed the best overall performance\")\n",
    "    print(f\"   Consider using this for production deployment\")\n",
    "    \n",
    "    print(f\"\\nModel Improvement Suggestions:\")\n",
    "    print(f\"   Hyperparameter tuning using GridSearchCV\")\n",
    "    print(f\"   Feature selection to reduce overfitting\")\n",
    "    print(f\"   Ensemble methods combining top 3 models\")\n",
    "    print(f\"   Cross-validation with different strategies\")\n",
    "    \n",
    "    print(f\"\\nData Collection Recommendations:\")\n",
    "    print(f\"   Collect more data to improve model robustness\")\n",
    "    print(f\"   Include additional relevant features\")\n",
    "    print(f\"   Address class imbalance if present\")\n",
    "    \n",
    "    print(f\"\\nDeployment Considerations:\")\n",
    "    print(f\"   Implement model monitoring\")\n",
    "    print(f\"   Set up automated retraining pipeline\")\n",
    "    print(f\"   Create prediction confidence intervals\")\n",
    "    \n",
    "    print(f\"\\nBusiness Impact:\")\n",
    "    avg_accuracy = metrics_summary['Accuracy'].mean()\n",
    "    print(f\"   Average model accuracy: {avg_accuracy:.1%}\")\n",
    "    print(f\"   Potential for automated migraine prediction\")\n",
    "    print(f\"   Can assist in preventive healthcare decisions\")\n",
    "    \n",
    "    # Specific recommendations based on results\n",
    "    print(f\"\\nSpecific Recommendations:\")\n",
    "    if best_model_name == \"Random Forest\":\n",
    "        print(f\"   Random Forest shows good performance and interpretability\")\n",
    "        print(f\"   Consider feature importance for clinical insights\")\n",
    "    elif best_model_name == \"Logistic Regression\":\n",
    "        print(f\"   Logistic Regression provides good baseline and interpretability\")\n",
    "        print(f\"   Consider regularization for better generalization\")\n",
    "    elif best_model_name == \"SVM\":\n",
    "        print(f\"   SVM shows good performance but may be slower for large datasets\")\n",
    "        print(f\"   Consider kernel selection for better performance\")\n",
    "\n",
    "provide_recommendations(metrics_summary, best_model_name, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 30)\n",
    "print(\"This comprehensive analysis successfully:\")\n",
    "print(\"Preprocessed and cleaned the migraine dataset\")\n",
    "print(\"Trained and evaluated 10 different ML models\")\n",
    "print(\"Identified the best performing model\")\n",
    "print(\"Provided actionable insights and recommendations\")\n",
    "print(\"\\nThe models are ready for deployment and further optimization!\")\n",
    "\n",
    "# Save the best model\n",
    "import joblib\n",
    "joblib.dump(models[best_model_name], f'best_model_{best_model_name.replace(\" \", \"_\").lower()}.pkl')\n",
    "print(f\"\\nBest model saved as: best_model_{best_model_name.replace(' ', '_').lower()}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': None, 'n_estimators': 200}\n",
      "Best CV F1: 0.8660142093857431\n"
     ]
    }
   ],
   "source": [
    "# Example: Grid Search for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3, scoring='f1_weighted')\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV F1:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
